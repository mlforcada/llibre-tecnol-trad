\chapter[Evaluación de los sistemas de TA]{Evaluación de los sistemas de traducción automática} \label{se:ASTA} 

Este capítulo pretende enunciar y describir muy brevemente algunos de los aspectos relevantes de la evaluación de los sistemas de traducción automática y dar algunas referencias que puedan ser de interés para quien quiera profundizar en este tema. 

\section{Cuestiones básicas} Cuando nos planteamos la evaluación de los sistemas de traducción automática (TA), hay algunas preguntas básicas que hay que responder. \citet{arnold94b} plantean el problema así: \begin{itemize} \item¿Cómo se puede decidir si un sistema de TA es \emph{bueno}? \item¿ Cómo se puede decidir si un sistema de TA es \emph{mejor} que otro? \end{itemize} y añaden la pregunta clave: ¿``Qué quiere decir \emph{bueno} o \emph{mejor} en este contexto''? La respuesta a todas estas preguntas es muy difícil, como dice \citet{minnis94j}: ``el hecho que no se haya propuesto ningún método de evaluación o de medición estándar es un buen indicador de la magnitud del problema''. 

Un concepto clave es el de \emph{utilidad}. La traducción automática será \emph{mejor} o \emph{de más calidad} cuanto más \emph{útil} sea para un propósito previsto. La \emph{utilidad} depende de la aplicación. Si la traducción automática se usa para la diseminación, es decir, como base para producir un texto adecuado para ser publicado, será más útil cuanto menos esfuerzo sea necesario para convertirla en adecuada (posteditarla). Pero si la traducción automática se usa tal como está para una aplicación de asimilación, es decir, para comprender un texto escrito en otra lengua, la utilidad aumenta con su inteligibilidad. 

\section{Tipos de evaluación} \label{ss:tipusaval} La naturaleza de la evaluación de un sistema de TU depende de varios factores: \begin{enumerate} \item \emph{Para que} se hace la evaluación? \citet{hutchins96u} distingue entre tres tipos básicos de evaluación: \begin{itemize} \item la \textbf{evaluación de adecuación}, que sirve para ``determinar la idoneidad [utilidad] de los sistemas de TU en un contexto operacional especificado'' ---por ejemplo, para decidir si el sistema de TU es útil para traducir el correo comercial de una empresa alimentaria---; \item la \textbf{evaluación diagnóstica}, que sirve para ``identificar limitaciones, errores o deficiencias, las cuales pueden ser corregidas o mejoradas'' ---por ejemplo, defectos en el tratamiento de la concordancia verbal de las oraciones subordinadas---, y \item la  \textbf{evaluación de funcionamiento}, ``para valorar el estado de desarrollo del sistema o las diferentes realizaciones técnicas'' ---por ejemplo, si el programa es robusto, rápido, hace un uso racional de la memoria del sistema, etc. \end{itemize} 

\item \emph{¿Quién} hace la evaluación? La evaluación la pueden hacer: \begin{enumerate} \item las personas que presumiblemente usarán el sistema o lo adquirirán para una empresa (evaluación de adecuación) o profesionales externos (\emph{consultores}) contratados al efecto; \item los investigadores, equipos de desarrollo, programadores (evaluación diagnóstica), muy especialmente durante el desarrollo de un sistema de TU; \item cualquiera de los dos grupos anteriores (evaluación del funcionamiento). \end{enumerate} \item \emph{¿Cómo} se hace la evaluación? Cuando se evalúa un sistema de TU se tienen en cuenta: \begin{enumerate} \item \emph{La calidad de las traducciones en bruto} producidas por el sistema. Tradicionalmente, la calidad se ha considerado de manera relativamente desconectada de las aplicaciones concretas, y se ha visto como una combinación (en proporciones difíciles de determinar\footnote{\citet{minnis94j} dice: ``La razón por la cual la medición de la calidad es difícil es, por supuesto, el hecho que la calidad sea un concepto tan polifacético e intangible''.}) de varios factores, como por ejemplo: la \emph{inteligibilidad} de los documentos traducidos por parte de los usuarios; la \emph{precisión} o \emph{fidelidad} con que el texto traducido comunica el significado del documento original (las cuales tienen que ser juzgadas por parte de personas bilingües conocedoras de la temática de los documentos); la \emph{naturalidad} o \emph{gramaticalidad} del texto; la adecuación del estilo o del registro de los documentos traducidos, etc. 

Esta evaluación se suele hacer mediante el uso de colecciones de documentos típicos o representativos (cómo se suele hacer en las evaluaciones de adecuación) o mediante series de pruebas objetivas (en inglés \emph{test suites}), usadas en las evaluaciones diagnósticas\footnote{Pero no únicamente, como indica \citet{lewis97j}, puesto que también pueden servir para que los usuarios juzguen la adecuación de la salida producida por el sistema.} y diseñadas para abarcar conjuntos completos de fenómenos lingüísticos que se manifiestan en la traducción.\footnote{Por ejemplo, el reordenamiento de las palabras de los sintagmas nominales cuando se traduce del inglés al español \citep{mira98j,forcada00}.} 

Por otro lado, siempre se debe tener en cuenta que los métodos de evaluación de la calidad dependen del uso que se piensa dar al sistema de TU \citep{arnold93j}; cómo se ha discutido en el capítulo~\ref{se:TiTA}, la noción central es la de \emph{propósito} de la traducción: \begin{itemize} \item La \textbf{evaluación} de un sistema que se usa \textbf{para la \emph{diseminación}} de textos se tiene que hacer estimando de alguna manera el esfuerzo de postedición, puesto que el sistema será tanto más útil cuanto más reducido sea este esfuerzo. 

Una posible medida cuantitativa de la calidad que aproxima \cite[p. 264]{sager93b} el esfuerzo de postedición por parte de profesionales de la traducción es la \emph{tasa de error por palabra} (o tasa de palabras corregidas). Esta medida se tiene que calcular sobre un conjunto suficientemente grande de textos \emph{representativos} de la tarea de traducción y se calcula como el porcentaje de inserciones, borrados y sustituciones de palabras que son estrictamente necesarios para transformar la traducción automática en bruto en una traducción adecuada al propósito. Esta medida tiene el inconveniente de que da la misma importancia a todas las operaciones de corrección, independientemente de la palabra; ello puede no ser adecuado porque el esfuerzo necesario para corregir todas las palabras no es el mismo.\footnote{Por ejemplo, no es lo mismo corregir un artículo que ha sido mal concordado con el sustantivo a que acompaña, que un término de especialidad, cuya corrección puede requerir que nos documentemos anteriormente.} 

Otra medida del esfuerzo de postedición es el tiempo que se tarda al posteditar una traducción automática para hacerla adecuada al propósito previsto. Medir el tiempo de postedición tiene el inconveniente que no todos los posteditores son igualmente eficientes ni tienen la misma experiencia posteditando. 

\item La \textbf{evaluación} de un sistema usado \textbf{para la \emph{asimilación}} de información se tiene que hacer de manera diferente: aquí la utilidad está relacionada más con la inteligibilidad, y se podría determinar directamente a través de cuestionarios de comprensión \citep{jones2007} o similares \citep{oregan13p,ageeva15p} o indirectamente, por ejemplo, estudiando el éxito en la hora de ejecutar una tarea con las instrucciones traducidas automáticamente \citep{doherty12p}. \end{itemize} \item \emph{La facilidad de uso del sistema de TU mismo}: por ejemplo, ``la facilidad con que se pueden crear y actualizar diccionarios, posteditar los textos, controlar el lenguaje de entrada'' o ``la extensibilidad [del sistema] a pares nuevos de idiomas o a nuevas temáticas'' \citep{hutchins96uno}. \end{enumerate} \end{enumerate} 

\begin{persabermes}{los problemas de posteditar para determinar la calidad} La determinación de la calidad de una traducción en bruto mediante el cómputo del número de correcciones necesarias no está exenta de problemas: \begin{itemize} \item Si suponemos que existe una única traducción aceptable del texto origen (lo que es mucho suponer) y lo usamos como referencia, existe más de una manera de corregir la traducción en bruto de forma que el resultado sea idéntico al de referencia. Para poder hacer comparaciones, estamos interesados en la corrección producida con el número mínimo de operaciones de inserción, borrado y sustitución de palabras; este número mínimo se puede considerar una \emph{distancia}, y, de hecho, matemáticamente, lo es: se denomina \emph{distancia de edición} (en inglés \emph{edit distance}). La búsqueda de eesta manera óptima de corregir puede no ser trivial para una persona, especialmente si los errores aparecen juntos y agrupados. \item Pero es que, además, la traducción de referencia puede no estar disponible; además, en la mayoría de los casos no hay una única traducción aceptable. De nuevo, si queremos comparar, querríamos encontrar la traducción aceptable más próxima a la traducción en bruto, es decir, la que se obtiene con el mínimo de correcciones posibles. Evaluar (corregir) la traducción en bruto comporta por lo tanto hacer una doble búsqueda: la persona que corrige tiene que buscar mentalmente la traducción aceptable más cercana (teniendo en cuenta los criterios que hacen aceptable una traducción, los cuales pueden no ser fáciles de aplicar), pero la \emph{distancia} entre los dos textos también se calcula haciendo una búsqueda mental del número mínimo de correcciones necesarias. \end{itemize} El hecho que sea posible que la evaluación por recuento de correcciones no sea óptima en vista de estos problemas hace que, además, sea especialmente difícil comparar las evaluaciones hechas por personas diferentes. Además, este tipo de evaluación es bastante costoso, puesto que para obtener una medición fiable de la calidad es necesario corregir textos de miles de palabras. \end{persabermes} 

\subsection{Análisis de costes y beneficios} \label{ss:costdetall} En el caso concreto de una aplicación de diseminación, finalmente, desde un punto de vista económico, lo que es relevante a la hora de decidir si se adopta o no un sistema de traducción automática es \emph{una comparación de los costes y de los beneficios} de usar este sistema de TU en lugar de usar exclusivamente los servicios de profesionales de la traducción: por ejemplo, si cuesta más (en gastos de personal) la postedición (revisión) de los textos meta producidos por el sistema (añadiendo el coste de usar el sistema de TA) que la traducción completa de los textos origen por parte de profesionales, entonces la adopción del sistema de TA no conviene a una empresa. Para ampliar la primera aproximación mencionada en la página~\pageref{pg:cost}, $$\textbf{coste}\left(\mbox{\begin{tabular}{c}traducción automática\\su +\\ postedición\end{tabular}}\right) < \textbf{coste}(\mbox{traducción profesional}), $$ tendríamos que considerar otros factores, es decir, todos los gastos en los que se incurre cuando se adopta la traducción automática seguida de postedición: \begin{itemize} \item \textbf{Costes de \emph{funcionamiento}} (coste efectivo por palabra), que tiene que tener en cuenta: \begin{itemize} \item la amortización del traductor automático (en caso de adquisición) \item el servicio técnico y el mantenimiento del sistema \item la migración (adaptación de los programas que se usan, la adquisición de sistemas informáticos) \end{itemize} 

\item \textbf{Costes de \emph{preedición} y de preparación}: hay que preparar y quizás preeditar (véase el apartado~\ref{ss:preedposted}) los textos que se tienen que traducir 

\item \textbf{Costes de postedición}: depende de la \emph{calidad} del texto en bruto y de la formación de los posteditores, a quien se puede pagar por horas de trabajo, por cantidad de texto corregido, etc. 

\item \textbf{Costes de formación}, puesto que los profesionales deben aprender a usar una nueva tecnología: \begin{itemize} \item \textbf{Formación en el uso del programa de traducción automática}: los profesionales tienen que aprender a usar, configurar y quizás mantener el nuevo software asociado \item \textbf{Formación en postedición,} la cual tiene que permitir que los profesionales \begin{itemize} \item conozcan el comportamiento del programa de traducción automática (por ejemplo, cuáles son los errores típicos que comete); \item aprendan técnicas de corrección, como por ejemplo el uso avanzado del procesador de textos (macroinstrucciones, sustitución de patrones, etc.) \end{itemize} \end{itemize} \end{itemize} 

%Z%Z%
\section[Traducción automática y traducción humana]{Sobre la comparación entre traducción automàtica y traducción humana} \label{ss:humaut} Una visión predominante de la evaluación de los sistemas de TU es la llamada \emph{metáfora del traductor humano}, según la cual \citep{krauwer93j} la tarea consiste en ``determinar hasta qué punto los constructores del sistema han conseguido imitar el comportamiento de un traductor humano''. \citet[p.~262]{sager93b} lo formula diciendo que ``se ha argumentado que la calidad de los documentos producidos mediante traducción automática se debería evaluar en términos de la identidad con productos humanos''. 

Tanto \citet{krauwer93j} como \citet{sager93b} cuestionan esta visión; este último argumenta que ``se debe aceptar que no hay ninguna situación que pueda servir como punto de comparación entre la traducción humana y la automática, y que quizás no hay ninguna situación en la cual la traducción humana y la automática sean igualmente adecuadas'' \citep[p.~261]{sager93b} y propone que, en cambio, las traducciones pueden ser comparadas para ver ``si satisfacen, y hasta qué punto, las expectativas del usuario final [de los documentos traducidos]'', puesto que la traducción es una ``actividad de mediación, cuya forma particular está determinada tanto por el texto como por las circunstancias comunicativas que requieren esta mediación'' \citep[p.~261]{sager93b}. En concreto, la traducción automática puede ser la más adecuada en algunas circunstancias, en vista de la enorme demanda general existente y, más concretamente, de la demanda de traducciones rápidas y baratas que no pueden ser producidas por profesionales. 

\begin{persabermes}{evaluación: evaluación predictiva} Hay un tipo de evaluación que se puede considerar como un caso particular de la evaluación diagnóstica definida en el apartado~\ref{ss:tipusaval}, aunque no se use estrictamente para mejorar el funcionamiento de un sistema, sino sólo para predecir el comportamiento del sistema en situaciones nuevas. Lo denominaremos aquí \emph{evaluación predictiva}, y se aplica principalmente a los sistemas de TA basados en reglas. 

Para poder hacer la evaluación predictiva, es crucial que los evaluadores tengan, en primer lugar, un modelo que describa aproximadamente el funcionamiento del sistema de traducción automática (relacionado con la tipología del sistema, es decir, de transferencia morfológica, sintáctica, etc., véase el cap.~\ref{se:TdTA}), y, en segundo lugar, un conjunto de textos o frases de evaluación (en inglés, \emph{test suite}) que les permita obtener detalles concretos sobre los datos lingüísticos (p.ej., las reglas) que usa aquel modelo. Las predicciones serían del estilo de ``cómo parece usar regles patrón--acción del estilo de ``si encuentra un patrón $X$, hará la acción $Y$'' y en una serie de casos encuentra el patrón $X_1$ y hace la acción $Y_1$, podemos predecir que siempre que encuentre este mismo patrón hará la misma acción''. Como la mayoría de los sistemas comerciales no nos dan suficiente información sobre la naturaleza del modelo, deberemos tratarlos como una \emph{caja negra}; la intuición de la persona evaluadora, su conocimiento de otros sistemas o de la historia de las empresas involucradas (por ejemplo, en cuanto a la adquisición de tecnología de otras empresas) y su habilidad para elegir ejemplos reveladores le permitirán determinar aspectos básicos del modelo de traducción (normalmente los ejemplos donde el sistema no traduce adecuadamente dan mucha más información que los ejemplos que se traducen adecuadamente). En particular, cualquier evaluación predictiva necesita tener una idea clara sobre el nivel de análisis que se hace en el sistema de TA, puesto que el nivel de análisis es el que más determina la naturaleza de un sistema (véase el apartado~\ref{ss:classtrans}). 

% L'avaluació predictiva pot tenir, a més, un paper fonamental en
% l'educació dels futurs traductors \citep{mira98j,forcada00p}, en
% vista de la disponibilitat creixent de sistemes comercials (per
% exemple, a través d'Internet).
Por otro lado, para que la evaluación sea útil los conjuntos de prueba deberían estar diseñados de forma que abarcaran conjuntos completos de fenómenos lingüísticos que se manifiesten con frecuencia relevante en las situaciones reales de traducción que se quiere evaluar, puesto que se quiere predecir el comportamiento del sistema en estas situaciones concretas. 

Existe una relación muy estrecha entre las técnicas descritas y la llamada \emph{ingeniería inversa}, o determinación detallada de la estrategia usada por un programa (en este caso, de traducción automática) para reprogramarla en otro. \end{persabermes} 

\section{Cuestiones y ejercicios} 

\begin{enumerate} \item ¿Qué característica de un sistema de traducción automática se debe considerar como especialmente importante cuando se evalúa la aplicación del sistema a la \emph{asimilación}? \begin{enumerate} \item sus útiles de postedición asistida. \item sus útiles de preedición asistida. \item la velocidad de respuesta. \end{enumerate} 

\item ¿Por qué es difícil evaluar la calidad de una traducción automática contando la cantidad mínima de postedición necesaria para hacerlo adecuado cuando no hay una traducción de referencia? \begin{enumerate} \item No es que sea difícil; sin traducción de referencia es absolutamente imposible. \item Porque esta tarea no se puede hacer sin conocer profundamente la estrategia usada por el sistema de traducción automática. \item Es relativamente sencillo corregir el texto porque sea adecuado pero es muy difícil hacerlo haciendo el mínimo número de cambios necesarios. \end{enumerate} 

\item Cuando se quiere usar un sistema de traducción automática para la \emph{asimilación} de información, ¿a que daríais \emph{menos} peso en la evaluación? \begin{enumerate} \item Facilidad de postedición de la traducción en bruto. \item Inteligibilidad de la traducción en bruto. \item Velocidad. \end{enumerate} 

\item Elegid la respuesta errónea. A la hora de decidir la adopción de un sistema de traducción automática para la postedición \ldots\begin{enumerate} \item \ldots tendremos que hacer una evaluación con textos parecidos a los que hay que traducir. \item \ldots los textos a usar en la evaluación deberán tener un número de palabras suficiente que nos permita extrapolar los resultados al resto de textos. \item \ldots deberemos evaluar, mediante cuestionarios o un método equivalente, la inteligibilidad de los textos traducidos en bruto. \end{enumerate} 

\item Estáis evaluando un sistema de traducción automática. Indicad cuál de estas tres magnitudes no usaríais como indicador directo del esfuerzo de postedición. \begin{enumerate} \item La inteligibilidad del texto meta en bruto. \item El número mínimo de palabras que se debe cambiar en el texto meta en bruto para hacerlo adecuado al propósito previsto, expresado como porcentaje respecto del número total de palabras. \item El tiempo necesario que se debe invertir para convertir el texto meta en bruto en un texto adecuado al propósito previsto, expresado como minutos por cada 1.000 palabras. \end{enumerate} 

\end{enumerate} 

En cuanto al concepto de \emph{evaluación predictiva}, mirad además los ejercicios~\ref{ex:cascat}, \ref{ex:zkanagg} y \ref{ex:postres} del  capítulo~\ref{se:TdTA}. 

\section{Soluciones} 

\begin{enumerate} \item (c) \item (c) \item (a) \item (c) \item (a) \end{enumerate} 

